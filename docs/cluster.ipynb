{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d61ed362aa666647e884e856f8a6ce510649070e79e0263068f970fc17431b76"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Clustering\n",
    "This notebook is to demonstrate the function of gathering the data from the stock market and creating the clustering algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering():\n",
    "    '''\n",
    "\n",
    "    Class to gather stock dataset and training the model.\n",
    "\n",
    "    methods:\n",
    "    - gather_data\n",
    "    - cluster_export\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, stock_list):\n",
    "        self.stock_list = stock_list\n",
    "    \n",
    "    def __get_increment(self, symbol):\n",
    "        '''\n",
    "\n",
    "        Get the increment of the stock in 1 month period.\n",
    "\n",
    "        return: Float increment.\n",
    "\n",
    "        '''\n",
    "\n",
    "        stock_data = yf.download(\n",
    "            tickers=symbol,\n",
    "            period='1mo',\n",
    "            interval='1wk'\n",
    "        )\n",
    "\n",
    "        stock_data = stock_data['Adj Close']\n",
    "\n",
    "        latest_price = stock_data[-1]\n",
    "        initial_price = stock_data[0]\n",
    "        \n",
    "        if initial_price is None:\n",
    "            initial_price = stock_data[1]\n",
    "        \n",
    "        if latest_price is None:\n",
    "            latest_price = stock_data[-2]\n",
    "\n",
    "        increment = (latest_price - initial_price) / initial_price\n",
    "\n",
    "        return increment\n",
    "\n",
    "    def gather_data(self):\n",
    "        '''\n",
    "\n",
    "        Gathering and formatting stock data into a dataframe and export it to CSV file.\n",
    "\n",
    "        return: None.\n",
    "\n",
    "        '''\n",
    "        list_columns = [\n",
    "            'symbol',\n",
    "            'sector',\n",
    "            'fullTimeEmployees',\n",
    "            'trailingAnnualDividendYield',\n",
    "            'payoutRatio',\n",
    "            'averageDailyVolume10Day',\n",
    "            'trailingAnnualDividendRate',\n",
    "            'averageVolume10days',\n",
    "            'dividendRate',\n",
    "            'beta',\n",
    "            'priceHint',\n",
    "            'trailingPE',\n",
    "            'regularMarketVolume',\n",
    "            'marketCap',\n",
    "            'averageVolume',\n",
    "            'priceToSalesTrailing12Months',\n",
    "            'forwardPE',\n",
    "            'fiveYearAvgDividendYield',\n",
    "            'dividendYield',\n",
    "            'enterpriseToRevenue',\n",
    "            'profitMargins',\n",
    "            'enterpriseToEbitda',\n",
    "            'forwardEps',\n",
    "            'bookValue',\n",
    "            'sharesPercentSharesOut',\n",
    "            'heldPercentInstitutions',\n",
    "            'netIncomeToCommon',\n",
    "            'trailingEps',\n",
    "            'lastDividendValue',\n",
    "            'priceToBook',\n",
    "            'heldPercentInsiders',\n",
    "            'shortRatio',\n",
    "            'enterpriseValue',\n",
    "            'earningsQuarterlyGrowth',\n",
    "            'pegRatio',\n",
    "            'shortPercentOfFloat',\n",
    "            'increment'\n",
    "        ]\n",
    "\n",
    "        df = pd.DataFrame(columns=list_columns)\n",
    "        for symbol in self.stock_list:\n",
    "            try:\n",
    "                print('[INFO] Gathering {} stock data...'.format(symbol))\n",
    "                ticker = yf.Ticker(symbol)\n",
    "\n",
    "                # Create DataFrame from Company Info\n",
    "                company_info = ticker.info\n",
    "                df_info = pd.DataFrame.from_dict(company_info, orient='index').T\n",
    "\n",
    "                df_info['symbol'] = [symbol]\n",
    "\n",
    "                # Get increment\n",
    "                increment = self.__get_increment(symbol)\n",
    "                df_info['increment'] = [increment]\n",
    "\n",
    "                # Remove unnecessary column\n",
    "                df_info.drop(df_info.columns.difference(list_columns), 1, inplace=True)\n",
    "\n",
    "                # Add to the main dataframe\n",
    "                df = df.append(df_info, ignore_index=True)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Fill NaN value to 0\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        export = df.to_csv('Stock Dataset.csv', index=False)\n",
    "        print('[SUCCESS] Stock Dataset has been exported to the local directory.')\n",
    "\n",
    "    def normalize_dataset(self, path, index='symbol'):\n",
    "        '''\n",
    "\n",
    "        Cluster the dataset and export to a new csv dataset with clustered value.\n",
    "\n",
    "        @param path: Folder directory of the dataset.\n",
    "        @param index: Name of the column of index.\n",
    "\n",
    "        return: None.\n",
    "\n",
    "        '''\n",
    "\n",
    "        df = pd.read_csv(path, index_col=index)\n",
    "\n",
    "        # Remove inf and nan row\n",
    "        df = df.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "\n",
    "        # Store columns name and index\n",
    "        columns = df.columns\n",
    "        index_column = df.index\n",
    "\n",
    "        df_values = df.values\n",
    "\n",
    "        # Define scaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        df_values_scaled = scaler.fit_transform(df_values)\n",
    "\n",
    "        df = pd.DataFrame(df_values_scaled, columns=columns, index=index_column)\n",
    "\n",
    "        export = df.to_csv('Normalized Stock Dataset.csv')\n",
    "        print('[SUCCESS] Normalized Stock Dataset has been exported to the local directory.')\n",
    "\n",
    "    def cluster_export(self, path, index='symbol', n_clusters=500):\n",
    "        '''\n",
    "\n",
    "        Cluster the dataset and export to a new csv dataset with clustered value.\n",
    "\n",
    "        @param path: Folder directory of the normalized dataset.\n",
    "        @param index: Name of the column of index.\n",
    "        @param n_clusters: Number of cluster\n",
    "\n",
    "        return: None.\n",
    "\n",
    "        '''\n",
    "\n",
    "        df = pd.read_csv(path, index_col=index)\n",
    "\n",
    "        model = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "        model = model.fit(df.values)\n",
    "\n",
    "        # Cluster labels\n",
    "        labels = model.labels_\n",
    "\n",
    "        df['cluster'] = labels\n",
    "\n",
    "        df = df['cluster']\n",
    "\n",
    "        export = df.to_csv('Clustered Stock Dataset.csv')\n",
    "        print('[SUCCESS] Normalized Stock Dataset has been exported to the local directory.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] Gathering AAPL stock data...\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[INFO] Gathering MSFT stock data...\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[SUCCESS] Stock Dataset has been exported to the local directory.\n"
     ]
    }
   ],
   "source": [
    "with open('tickers.json', 'r') as f:\n",
    "    json_tickers = f.read()\n",
    "    list_tickers = json.loads(json_tickers)\n",
    "\n",
    "test_tickers = ['AAPL', 'MSFT']\n",
    "\n",
    "clustering = Clustering(list_tickers)\n",
    "\n",
    "# Run to gather the data\n",
    "clustering.gather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            sector  fullTimeEmployees  trailingAnnualDividendYield  \\\nsymbol                                                               \nAAPL    Technology             147000                     0.006668   \nMSFT    Technology             163000                     0.009077   \n\n        payoutRatio  averageDailyVolume10Day  trailingAnnualDividendRate  \\\nsymbol                                                                     \nAAPL         0.2177                123191000                       0.807   \nMSFT         0.3115                 32066483                       2.140   \n\n        averageVolume10days  dividendRate      beta  priceHint  ...  \\\nsymbol                                                          ...   \nAAPL              123191000          0.82  1.251354          2  ...   \nMSFT               32066483          2.24  0.812567          2  ...   \n\n        trailingEps  lastDividendValue  priceToBook  heldPercentInsiders  \\\nsymbol                                                                     \nAAPL          3.687              0.205    31.501522              0.00065   \nMSFT          6.707              0.560    13.605075              0.00059   \n\n        shortRatio  enterpriseValue  earningsQuarterlyGrowth  pegRatio  \\\nsymbol                                                                   \nAAPL          0.97    2067080282112                    0.293      2.02   \nMSFT          1.36    1728870350848                    0.327      1.82   \n\n        shortPercentOfFloat  increment  \nsymbol                                  \nAAPL                 0.0060  -0.045276  \nMSFT                 0.0055  -0.023319  \n\n[2 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Stock Dataset.csv', index_col='symbol')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 36)) while a minimum of 1 is required by StandardScaler.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-82b048438eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Stock Dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Normalized Stock Dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_normalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-1434c2526c83>\u001b[0m in \u001b[0;36mnormalize_dataset\u001b[0;34m(self, path, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdf_values_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_values_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \"\"\"\n\u001b[1;32m    762\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0m\u001b[1;32m    764\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                                 force_all_finite='allow-nan', reset=first_call)\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'no_validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[1;32m    670\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 36)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "clustering.normalize_dataset('Stock Dataset.csv')\n",
    "\n",
    "df_normalized = pd.read_csv('Normalized Stock Dataset.csv')\n",
    "\n",
    "print(df_normalized.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[SUCCESS] Normalized Stock Dataset has been exported to the local directory.\n"
     ]
    }
   ],
   "source": [
    "clustering.cluster_export('Normalized Stock Dataset.csv', n_clusters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  symbol  cluster\n0      A        0\n1     AA        0\n2    AAA        3\n3   AAAU        3\n4   AACG       11\n"
     ]
    }
   ],
   "source": [
    "df_clustered = pd.read_csv('Clustered Stock Dataset.csv')\n",
    "\n",
    "print(df_clustered.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "symbol_cluster = df_clustered.loc[df_clustered['symbol'] == 'JPM']['cluster'].values[0]\n",
    "print(symbol_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     symbol  cluster\n692     BAC        7\n1181      C        7\n1538  CMCSA        7\n1723   CSCO        7\n3574     HD        7\n4094   INTC        7\n4374    JNJ        7\n4394    JPM        7\n4534     KO        7\n4863     MA        7\n5171    MRK        7\n5185     MS        7\n5602    NVS        7\n5774   ORCL        7\n5966    PEP        7\n5981    PFE        7\n6011     PG        7\n7932    UNH        7\n8028      V        7\n8311     VZ        7\n"
     ]
    }
   ],
   "source": [
    "same_cluster = df_clustered.loc[df_clustered['cluster'] == symbol_cluster]\n",
    "print(same_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}